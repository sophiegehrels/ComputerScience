{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This bootstrap is used to retrieve the F1 values of the old approach. Hence the old data cleaning method is used (as in the MSMP+ method), \n",
    "#and the 'old' way of creating binary vectors is used (as in MSMP+).\n",
    "\n",
    "from data_cleaning import clean_tv_data as old_clean\n",
    "from binary_vectors import * \n",
    "from minhashing import *\n",
    "from lsh import *\n",
    "from true_pairs import *\n",
    "from plotting import *\n",
    "from msm import *\n",
    "from distance_matrix import *\n",
    "from f1_scores import *\n",
    "from utilities import *\n",
    "import itertools\n",
    "\n",
    "input_file_path = \"TVs-all-merged.json\"\n",
    "output_file_path_old = \"TVs-all-merged-cleaned_old.json\"\n",
    "old_clean(input_file_path, output_file_path_old)\n",
    "\n",
    "with open(output_file_path_old, 'r') as file: \n",
    "    data_cleaned_old = json.load(file)\n",
    "\n",
    "all_products = [item for model_id, items in data_cleaned_old.items() for item in items]\n",
    "all_pairs = list(itertools.combinations(range(len(all_products)), 2))\n",
    "all_true_pairs = find_set_duplicates(all_products)\n",
    "\n",
    "# Parameters\n",
    "gamma = 0.7\n",
    "alpha = 0.6 \n",
    "beta = 0.3 \n",
    "mu_1 = 0.6\n",
    "mu_2 = 0.7\n",
    "delta = 0.7\n",
    "epsilon_TMWM = 0\n",
    "\n",
    "bootstrap_number = 5\n",
    "train_ratio = 0.63\n",
    "epsilon_range = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "F1_training_scores = {i: {} for i in range(bootstrap_number)}\n",
    "F1_testing_scores_old = {i: {} for i in range(bootstrap_number)}\n",
    "\n",
    "#Creat and save the distance matrix\n",
    "full_distance_matrix_1 = generate_distance_matrix_opt(all_products, all_pairs, gamma, alpha, beta, mu_1, delta, epsilon_TMWM)\n",
    "np.save(\"distance_matrix_mu0.6.npy\", full_distance_matrix_1)\n",
    "full_distance_matrix = np.load(\"distance_matrix_1312.npy\")\n",
    "\n",
    "for i in range(bootstrap_number): \n",
    "    num_products = len(all_products)\n",
    "    indices = list(range(num_products))\n",
    "\n",
    "    subset_size = int(num_products * train_ratio)\n",
    "    train_subset = random.sample(all_products, subset_size)\n",
    "    test_subset = [product for product in all_products if product not in train_subset]\n",
    "\n",
    "    train_num_products_subset = len(train_subset)\n",
    "    train_subset_indices = list(range(train_num_products_subset))\n",
    "    train_original_indices = [all_products.index(element) for element in train_subset]\n",
    "    train_index_mapping = {subset_index: original_index for subset_index, original_index in zip(train_subset_indices, train_original_indices)}\n",
    "    train_pairs, train_true_pairs, train_signature_matrix, n = get_signature_matrix(train_subset)\n",
    "\n",
    "    test_num_products_subset = len(test_subset)\n",
    "    test_subset_indices = list(range(test_num_products_subset))\n",
    "    test_original_indices = [all_products.index(element) for element in test_subset]\n",
    "    test_index_mapping = {subset_index: original_index for subset_index, original_index in zip(test_subset_indices, test_original_indices)}\n",
    "    test_pairs, test_true_pairs, test_signature_matrix, n = get_signature_matrix(test_subset)\n",
    "\n",
    "    for b in range(1, n): #     CHANGE TO N+1\n",
    "        if n % b != 0:  \n",
    "            continue  \n",
    "\n",
    "        r = n // b  \n",
    "        print(b,r)\n",
    "\n",
    "        F1_training_scores[i][b] = {epsilon_clustering: 0 for epsilon_clustering in epsilon_range}\n",
    "        F1_testing_scores_old[i][b] = {'F1': 0, 'fraction_of_comparisons': 0}\n",
    "\n",
    "        train_candidate_pairs = lsh(train_signature_matrix, b, r)\n",
    "        train_candidate_pairs_original = {(train_index_mapping[i], train_index_mapping[j]) for i, j in train_candidate_pairs}\n",
    "        train_true_pairs_original = {(train_index_mapping[i], train_index_mapping[j]) for i, j in train_true_pairs}\n",
    "\n",
    "        upper_triangular = np.triu(full_distance_matrix)\n",
    "        dm_symm = upper_triangular + upper_triangular.T - np.diag(np.diag(upper_triangular))\n",
    "        train_distance_matrix = generate_distance_matrix_candidates(dm_symm, train_candidate_pairs_original)\n",
    "                \n",
    "        for epsilon_clustering in epsilon_range: \n",
    "            F_1 = calculate_F1_score(train_true_pairs_original, epsilon_clustering, train_distance_matrix)\n",
    "            F1_training_scores[i][b][epsilon_clustering] = F_1\n",
    "\n",
    "        print(f\"TESTING: {b}, {r}\")\n",
    "        optimal_epsilon_for_ib = max(F1_training_scores[i][b], key=lambda epsilon: F1_training_scores[i][b][epsilon])\n",
    "        print(optimal_epsilon_for_ib)\n",
    "        test_candidate_pairs = lsh(test_signature_matrix, b, r)\n",
    "        test_candidate_pairs_original = {(test_index_mapping[i], test_index_mapping[j]) for i, j in test_candidate_pairs}\n",
    "        test_true_pairs_original = {(test_index_mapping[i], test_index_mapping[j]) for i, j in test_true_pairs}\n",
    "        test_distance_matrix = generate_distance_matrix_candidates(dm_symm, test_candidate_pairs_original)\n",
    "        \n",
    "        F1_testing_scores_old[i][b]['F1'] = calculate_F1_score(test_true_pairs_original, optimal_epsilon_for_ib, test_distance_matrix)\n",
    "        F1_testing_scores_old[i][b]['fraction_of_comparisons'] = len(test_candidate_pairs_original) / (test_num_products_subset * (test_num_products_subset - 1) / 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This bootstrap is used to retrieve the F1 values of the new approach. Hence the improved data cleaning method is used (imported as improved_clean), \n",
    "#and the improved way of creating binary vectors (with brand), imported as improved binary matrix. \n",
    "\n",
    "from data_cleaning_improved import clean_tv_data as improved_clean\n",
    "from binary_vectors_improved import obtain_binary_matrix as improved_binary_matrix\n",
    "from minhashing import *\n",
    "from lsh import *\n",
    "from true_pairs import *\n",
    "from plotting import *\n",
    "from msm import *\n",
    "from distance_matrix import *\n",
    "from f1_scores import *\n",
    "from utilities import *\n",
    "import itertools\n",
    "\n",
    "input_file_path = \"TVs-all-merged.json\"\n",
    "output_file_path_improved = \"TVs-all-merged-cleaned_improved.json\"\n",
    "improved_clean(input_file_path, output_file_path_improved)\n",
    "\n",
    "with open(output_file_path_improved, 'r') as file: \n",
    "    data_cleaned_improved = json.load(file)\n",
    "\n",
    "all_products = [item for model_id, items in data_cleaned_improved.items() for item in items]\n",
    "all_pairs = list(itertools.combinations(range(len(all_products)), 2))\n",
    "all_true_pairs = find_set_duplicates(all_products)\n",
    "\n",
    "# Parameters\n",
    "gamma = 0.7\n",
    "alpha = 0.6 \n",
    "beta = 0.3 \n",
    "mu_1 = 0.6\n",
    "mu_2 = 0.7\n",
    "delta = 0.7\n",
    "epsilon_TMWM = 0\n",
    "\n",
    "bootstrap_number = 5\n",
    "train_ratio = 0.63\n",
    "epsilon_range = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "F1_training_scores = {i: {} for i in range(bootstrap_number)}\n",
    "F1_testing_scores_new = {i: {} for i in range(bootstrap_number)}\n",
    "\n",
    "#Creat and save the distance matrix\n",
    "full_distance_matrix_1 = generate_distance_matrix_opt(all_products, all_pairs, gamma, alpha, beta, mu_1, delta, epsilon_TMWM)\n",
    "np.save(\"distance_matrix_mu0.6.npy\", full_distance_matrix_1)\n",
    "full_distance_matrix = np.load(\"distance_matrix_1312.npy\")\n",
    "\n",
    "for i in range(bootstrap_number): \n",
    "    num_products = len(all_products)\n",
    "    indices = list(range(num_products))\n",
    "\n",
    "    subset_size = int(num_products * train_ratio)\n",
    "    train_subset = random.sample(all_products, subset_size)\n",
    "    test_subset = [product for product in all_products if product not in train_subset]\n",
    "\n",
    "    train_num_products_subset = len(train_subset)\n",
    "    train_subset_indices = list(range(train_num_products_subset))\n",
    "    train_original_indices = [all_products.index(element) for element in train_subset]\n",
    "    train_index_mapping = {subset_index: original_index for subset_index, original_index in zip(train_subset_indices, train_original_indices)}\n",
    "\n",
    "    train_pairs = list(itertools.combinations(range(len(train_subset)), 2))\n",
    "    train_true_pairs = find_set_duplicates(train_subset)\n",
    "    train_binary_matrix = improved_binary_matrix(train_subset)\n",
    "    train_signature_matrix = minhash(train_binary_matrix)\n",
    "    rows, cols = train_signature_matrix.shape\n",
    "    n = rows\n",
    "\n",
    "    test_num_products_subset = len(test_subset)\n",
    "    test_subset_indices = list(range(test_num_products_subset))\n",
    "    test_original_indices = [all_products.index(element) for element in test_subset]\n",
    "    test_index_mapping = {subset_index: original_index for subset_index, original_index in zip(test_subset_indices, test_original_indices)}\n",
    "\n",
    "    test_pairs = list(itertools.combinations(range(len(test_subset)), 2))\n",
    "    test_true_pairs = find_set_duplicates(test_subset)\n",
    "    test_binary_matrix = improved_binary_matrix(test_subset)\n",
    "    test_signature_matrix = minhash(test_binary_matrix)\n",
    "    \n",
    "    for b in range(1, n): #     CHANGE TO N+1\n",
    "        if n % b != 0:  \n",
    "            continue  \n",
    "\n",
    "        r = n // b  \n",
    "        print(b,r)\n",
    "\n",
    "        F1_training_scores[i][b] = {epsilon_clustering: 0 for epsilon_clustering in epsilon_range}\n",
    "        F1_testing_scores_new[i][b] = {'F1': 0, 'fraction_of_comparisons': 0}\n",
    "\n",
    "        train_candidate_pairs = lsh(train_signature_matrix, b, r)\n",
    "        train_candidate_pairs_original = {(train_index_mapping[i], train_index_mapping[j]) for i, j in train_candidate_pairs}\n",
    "        train_true_pairs_original = {(train_index_mapping[i], train_index_mapping[j]) for i, j in train_true_pairs}\n",
    "\n",
    "        upper_triangular = np.triu(full_distance_matrix)\n",
    "        dm_symm = upper_triangular + upper_triangular.T - np.diag(np.diag(upper_triangular))\n",
    "        train_distance_matrix = generate_distance_matrix_candidates(dm_symm, train_candidate_pairs_original)\n",
    "                \n",
    "        for epsilon_clustering in epsilon_range: \n",
    "            F_1 = calculate_F1_score(train_true_pairs_original, epsilon_clustering, train_distance_matrix)\n",
    "            F1_training_scores[i][b][epsilon_clustering] = F_1\n",
    "\n",
    "        print(f\"TESTING: {b}, {r}\")\n",
    "        optimal_epsilon_for_ib = max(F1_training_scores[i][b], key=lambda epsilon: F1_training_scores[i][b][epsilon])\n",
    "        print(optimal_epsilon_for_ib)\n",
    "        test_candidate_pairs = lsh(test_signature_matrix, b, r)\n",
    "        test_candidate_pairs_original = {(test_index_mapping[i], test_index_mapping[j]) for i, j in test_candidate_pairs}\n",
    "        test_true_pairs_original = {(test_index_mapping[i], test_index_mapping[j]) for i, j in test_true_pairs}\n",
    "        test_distance_matrix = generate_distance_matrix_candidates(dm_symm, test_candidate_pairs_original)\n",
    "        \n",
    "        F1_testing_scores_new[i][b]['F1'] = calculate_F1_score(test_true_pairs_original, optimal_epsilon_for_ib, test_distance_matrix)\n",
    "        F1_testing_scores_new[i][b]['fraction_of_comparisons'] = len(test_candidate_pairs_original) / (test_num_products_subset * (test_num_products_subset - 1) / 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the  F1_testing_scores_new and  F1_testing_scores_old to create plots. We plot the F1 scores against the fraction of comparisons. \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Provided data\n",
    "data_old = F1_testing_scores_old\n",
    "data_new = F1_testing_scores_new\n",
    "\n",
    "def process_data(data):\n",
    "    averaged_data = {}\n",
    "    for i, b_data in data.items():\n",
    "        for b, values in b_data.items():\n",
    "            if b not in averaged_data:\n",
    "                averaged_data[b] = {'total_F1': 0, 'total_fraction': 0, 'count': 0}\n",
    "            averaged_data[b]['total_F1'] += values['F1']\n",
    "            averaged_data[b]['total_fraction'] += values['fraction_of_comparisons']\n",
    "            averaged_data[b]['count'] += 1\n",
    "\n",
    "    for b, values in averaged_data.items():\n",
    "        count = values['count']\n",
    "        averaged_data[b]['average_F1'] = values['total_F1'] / count\n",
    "        averaged_data[b]['average_fraction'] = values['total_fraction'] / count\n",
    "\n",
    "    averaged_fractions = [values['average_fraction'] for values in averaged_data.values()]\n",
    "    averaged_F1_scores = [values['average_F1'] for values in averaged_data.values()]\n",
    "\n",
    "    return averaged_fractions, averaged_F1_scores\n",
    "\n",
    "averaged_fractions, averaged_F1_scores = process_data(data_old)\n",
    "averaged_fractions_2, averaged_F1_scores_2 = process_data(data_new)\n",
    "\n",
    "# Plotting the averaged data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(averaged_fractions, averaged_F1_scores, color='orange', label='F1 Old')\n",
    "plt.plot(averaged_fractions_2, averaged_F1_scores_2, color='blue', label='F1 New')\n",
    "plt.xlabel('Average Fraction of Comparisons')\n",
    "plt.ylabel('Average F1 Score')\n",
    "plt.title('Average F1 Score vs Average Fraction of Comparisons (Averaged Over All Bootstraps)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
